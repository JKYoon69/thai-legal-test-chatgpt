import json
from pathlib import Path
import streamlit as st

from parser_core.parser import detect_doc_type, parse_document
from parser_core.postprocess import validate_tree, make_chunks
from parser_core.schema import ParseResult
from exporters.writers import to_jsonl, make_zip_bundle

st.set_page_config(page_title="Thai Law Parser (Test)", layout="wide")
st.title("ğŸ“œ Thai Law Parser â€” í…ŒìŠ¤íŠ¸")

with st.sidebar:
    st.markdown("**ì—…ë¡œë“œ â†’ íŒŒì‹± â†’ ê²€í†  â†’ ë‹¤ìš´ë¡œë“œ** ìˆœì„œë¡œ ì§„í–‰í•˜ì„¸ìš”.")
    st.markdown("ë¬¸ì„œìœ í˜• ê°ì§€ ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ìœ¼ë¡œ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”.")
    st.caption("v0.2 â€” line-anchored headers, spans-only nodes")

uploaded = st.file_uploader("íƒœêµ­ì–´ ë²•ë¥  ë¬¸ì„œ ì—…ë¡œë“œ (.txt)", type=["txt"])

if "raw_text" not in st.session_state:
    st.session_state.raw_text = None
    st.session_state.result: ParseResult|None = None

if uploaded:
    raw = uploaded.read().decode("utf-8", errors="ignore")
    st.session_state.raw_text = raw

if st.session_state.raw_text:
    raw_text = st.session_state.raw_text
    with st.expander("ì›ë¬¸ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
        st.text(raw_text[:1500] + ("..." if len(raw_text) > 1500 else ""))

    auto_type = detect_doc_type(raw_text)
    st.write(f"ğŸ” ìë™ ê°ì§€ëœ ë¬¸ì„œ ìœ í˜•: **{auto_type}**")

    forced_type = st.selectbox(
        "ë¬¸ì„œ ìœ í˜•(ìˆ˜ì • ê°€ëŠ¥)",
        options=["auto", "code", "act", "royal_decree", "regulation"],
        index=0,
        help="ê°ì§€ê°€ í‹€ë¦¬ë©´ ì—¬ê¸°ì„œ ê°•ì œë¡œ ì§€ì •í•˜ì„¸ìš”.",
    )
    if forced_type == "auto":
        forced_type = None

    if st.button("ğŸ§© íŒŒì‹± ì‹¤í–‰", use_container_width=True):
        with st.spinner("íŒŒì‹± ì¤‘..."):
            result = parse_document(raw_text, forced_type=forced_type)
            st.session_state.result = result

    result: ParseResult|None = st.session_state.result
    if result:
        st.success(
            f"íŒŒì‹± ì™„ë£Œ: ë…¸ë“œ {len(result.nodes)}ê°œ, ìµœí•˜ìœ„ ë…¸ë“œ {result.stats.get('leaf_count', 0)}ê°œ"
        )

        # ê²€ì¦ ë¦¬í¬íŠ¸
        issues = validate_tree(result)
        with st.expander(f"ğŸ” ì •í•©ì„± ê²€ì¦ (ê²½ê³  {len(issues)}ê±´)", expanded=False):
            if not issues:
                st.write("ë¬¸ì œ ì—†ìŒ âœ…")
            else:
                for i, iss in enumerate(issues, 1):
                    st.write(f"{i}. [{iss.level}] {iss.message}")

        # ì¢Œ/ìš° ì»¬ëŸ¼: ì¢Œ(íŠ¸ë¦¬), ìš°(í•˜ì´ë¼ì´íŠ¸)
        left, right = st.columns([1, 2], gap="large")

        with left:
            st.subheader("ê³„ì¸µ íŠ¸ë¦¬")
            def render_node(node, depth=0):
                pad = "ã€€" * depth
                label = f"{node.level} {node.label}".strip()
                st.write(f"{pad}- **{label}**  (chars {node.span.start}â€“{node.span.end})")
                for ch in node.children:
                    render_node(ch, depth + 1)

            for n in result.root_nodes:
                render_node(n, 0)

        with right:
            st.subheader("ì›ë¬¸ í•˜ì´ë¼ì´íŠ¸")
            leaf_options = [(n.node_id, f"{n.level} {n.label}") for n in result.nodes]
            sel = st.selectbox("ë…¸ë“œ ì„ íƒ(í•´ë‹¹ ë¶€ë¶„ í•˜ì´ë¼ì´íŠ¸)", leaf_options, index=0)
            sel_id = sel[0]
            target = next((n for n in result.nodes if n.node_id == sel_id), None)
            if target:
                start = max(0, target.span.start - 200)
                end = min(len(raw_text), target.span.end + 200)
                prefix = raw_text[start:target.span.start]
                body = raw_text[target.span.start:target.span.end]
                suffix = raw_text[target.span.end:end]
                st.markdown(
                    f"â€¦{prefix}<mark style='background-color:#fff2a8'>{body}</mark>{suffix}â€¦",
                    unsafe_allow_html=True,
                )

        st.divider()
        st.subheader("ì²­í‚¹ (RAG ì…ë ¥ìš©)")
        mode = st.selectbox("ë³‘í•© ëª¨ë“œ", ["article_only", "articleÂ±1"], index=1)
        chunks = make_chunks(raw_text, result, mode=mode)
        st.write(f"ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ")

        with st.expander("ì²­í¬ ë¯¸ë¦¬ë³´ê¸°(JSON)", expanded=False):
            st.code(json.dumps([c.model_dump() for c in chunks[:5]], ensure_ascii=False, indent=2))

        # ë‹¤ìš´ë¡œë“œ íŒŒì¼ ìƒì„± (nodesëŠ” í…ìŠ¤íŠ¸ ì œì™¸ë¼ ë§¤ìš° ì‘ìŠµë‹ˆë‹¤)
        out_dir = Path("out")
        out_dir.mkdir(exist_ok=True)
        jsonl_nodes = out_dir / "nodes.jsonl"
        jsonl_chunks = out_dir / "chunks.jsonl"
        preview_html = out_dir / "preview.html"
        zip_path = out_dir / "bundle.zip"

        to_jsonl(result.nodes, jsonl_nodes)
        to_jsonl(chunks, jsonl_chunks)

        preview_html.write_text(
            "<html><meta charset='utf-8'><body>"
            "<h3>Thai Law Parser Preview</h3>"
            f"<pre>{json.dumps([n.model_dump() for n in result.nodes[:30]], ensure_ascii=False, indent=2)}</pre>"
            "</body></html>",
            encoding="utf-8",
        )

        make_zip_bundle(zip_path, {
            "nodes.jsonl": jsonl_nodes.read_bytes(),
            "chunks.jsonl": jsonl_chunks.read_bytes(),
            "preview.html": preview_html.read_bytes(),
        })

        st.download_button("â¬‡ï¸ nodes.jsonl ë‹¤ìš´ë¡œë“œ", data=jsonl_nodes.read_bytes(),
                           file_name="nodes.jsonl", mime="application/jsonl")
        st.download_button("â¬‡ï¸ chunks.jsonl ë‹¤ìš´ë¡œë“œ", data=jsonl_chunks.read_bytes(),
                           file_name="chunks.jsonl", mime="application/jsonl")
        st.download_button("â¬‡ï¸ ë²ˆë“¤ ZIP ë‹¤ìš´ë¡œë“œ", data=zip_path.read_bytes(),
                           file_name="thai-law-parser-bundle.zip", mime="application/zip")
